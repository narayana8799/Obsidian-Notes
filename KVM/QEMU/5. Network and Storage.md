Virtual Networking
--
The main components of `libvirt` networking are bridge and tap devices. Bridges are like virtual switches that VM's connect to using virtual interfaces called TA devices. Bridges can be created using `brctl` and TAP devices can be created using `ip` command. Once a bridge and TAP are created they are connected.

```
// Create and Connect bridge and tap

- brctl addbr <brdge name>
- ip tuntap add dev <tap-name> mode tap
- brctl addif <bridge-name> <tap-name>

// Disconnect and delete bridge and tap

- brctl delif <bridge-name> <tap-name>
- ip tuntap del dev <tap-name> mode tap
- brctl delbr <bridge-name>
```

The types of virtual networking available are as follows:

- Isolated virtual network
- Routed virtual network
- NATed virtual network
- Bridged network using a physical NIC, VLAN interface, bond interface and bonded VLAN interface
- MacVTap
- PCI passthrough NPIV
- OVS

The process for creating a network is same for every type. It can either be done from `virt-manager` or using `virsh` with the help of XML file definition. For every network a bridge  is created by libvirt.

To connect a VM to a created network, a NIC should be added to the VM and options such as network, device type and device model should be selected. Once the hardware (NIC) is attached, it can be a attached and detached from the VM using following commands.

```
//Attachig 

virsh attach-interface --domain <vm-name> --type network --source <network-name> --model virtio --config --live

//Detaching

virsh detach-interface --domain <vm-name> --type network --mac <mac addr> --config --live

// --config makes the attachment persistent
// --live is used to connect an NIC to an actively running VM
```

![[Pasted image 20241120174813.png]]

#### Isolated Networks


![[Pasted image 20241120124818.png]]

In this configuration of networking, only virtual machines which are added to this network can communicate with each other. They cannot communicate with other networks. Network files are stored at `/etc/libvirt/qemu/networks`. 

#### Routed Networks

In a routed mode, the virtual network is connected to the physical network using
the IP routes specified on the hypervisor. These IP routes are used to route the
traffic from the virtual machines to the network attached to the hypervisor. The key
point that you need to remember with this configuration is that you need to set up
the correct IP route on your router or gateway devices also so that the reply packet
should reach the hypervisor back. If there are no routes defined, the reply packet will
never reach the host. This mode is not commonly used, unless you have a special
use case to create a network with this complexity.

![[Pasted image 20241120175653.png]]

#### NATed Networks

NATed mode is the most commonly used virtual networking when you want to set
up a test environment on your laptop or test machine. This mode allows the virtual
machines to communicate with the outside network without using any additional
configuration. This method also allows communication between the hypervisor and
the virtual machines. The major drawback of this virtual network is that none of the
systems outside the hypervisor can reach the virtual machines.

![[Pasted image 20241120182406.png]]


#### MacVTap Networks

MacVTap is used when you do not want to create a normal bridge, but want the
users in local network to access your virtual machine. This connection type is not
used in production systems and is mostly used on workstation systems. 


Storage
--
Disks can be created and attached to a VM at any time. Different types of storage type such as LVM, Network Share, Block Devices and many others are supported. Disks can be created using `dd` command. 

- **Pre-allocated:** Pre-allocated virtual disk allocates the space right away at the time of creation. A virtual disk with a pre-allocated format has significantly faster write speeds than a virtual disk with a thin provisioning. 
-  **Thin-Provisioned:** In this method, space will be allocated for the volume as needed. For example, if you create a 10G virtual disk (disk image) with sparse allocation. Initially, it would just take a couple of MB of space from your storage and grow as it receives write from the virtual machine up to 10G size. This allows storage over commitment under the assumption that the given disk space. To create a thin-provisioned disk, use the seek option with the dd command, as shown in the following command:

```
// Pre-allocated disk

--> dd if=/dev/zero of=/vms/disks/pre.img bs=1G count=10

// Thin Provisioning disk

--> dd if=/dev/zero of=/vms/disks/thin.img bs=1G count=0 seek=10

// To attach a disk to a live VM

--> virsh attach-disk <vm-name> </path/to/disk> <target-disk-name> --config --live
```

libvirt supports the following storage pool types:

- dir: Uses the filesystem directory to store virtual disks
- disk: Uses physical hard disks to create virtual disks
- fs: Uses pre-formatted partitions to store virtual disks
- netfs: Uses network-shared storage like NFS to store virtual disks
- gluster: Allows using the gluster filesystem to store virtual disks
- iscsi: Uses network-shared ISCSI storage to store virtual disks
- scsi: Uses local SCSI storage to store virtual disks
- lvm: Depends on LVM volume groups to store virtual disks
- rbd: Allows connecting ceph storage for virtual disks

The storage that is controlled and monitored by libvirt in terms of storage pools and storage volumes is called as managed storage here. A pool is a generic container for various storage objects. There are several types of storage pools. Starting from a simple local directory to advance network shares like ceph storage volumes are part of storage
pool and they are actually the virtual disks used by virtual machines.

To create a storage pool using `virsh` follow:

```
// To create a storage pool

- virsh pool-define-as <pool-name> dir - - - - "<location>"
- virsh pool-build <pool-name>
- virsh pool-start <pool-name>

// To deleate a storage pool

- virsh pool-destroy <pool-name>
- virsh pool-undefine <pool-name>

// To create and delete a volume

- virsh vol-create-as <pool-name> <vol-name> <size|M/G(10G)>
- virsh vol-delete <pool-name> <vol-name>
```

==`qemu-guest-agent` should run on VM's to information regarding them. It also plays a very important role in taking snapshots of VM's to preserver the integrity of the disks. It runs only om Linux machines.== `qemu-guest-agent` can be executed as mentioned below:

```
// Lits all available guest commands
- virsh qemu-agent-command ,vm-name '{"execute": "guest-info"}' --pretty
```

